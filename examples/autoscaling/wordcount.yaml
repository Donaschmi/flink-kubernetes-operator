################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: flink
spec:
  image: donaschmitz/justin:0.8
  flinkVersion: v1_18
  flinkConfiguration:
    kubernetes.operator.job.autoscaler.enabled: "true"
    kubernetes.operator.job.autoscaler.justin.enabled: "false"
    kubernetes.operator.job.autoscaler.stabilization.interval: "1m"
    kubernetes.operator.job.autoscaler.metrics.window: "2m"
    pipeline.max-parallelism: "24"
    taskmanager.numberOfTaskSlots: "4"
    state.savepoints.dir: file:///tmp/savepoints
    state.checkpoints.dir: file:///tmp/checkpoints
    high-availability.type: kubernetes
    high-availability.storageDir: file:///tmp/ha
    execution.checkpointing.interval: "1m"
    jobmanager.scheduler: adaptive
    kubernetes.operator.job.autoscaler.target.utilization.boundary: "0.1"
    metrics.reporters: prom
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
    #metrics.reporter.prom.port: 9250-9260
    web.submit.enable: "true"
    web.cancel.enable: "true"
    state.backend: rocksdb

    state.backend.rocksdb.use-direct-reads: "true"
    state.backend.rocksdb.use-direct-writes: "true"

    state.backend.rocksdb.metrics.block-cache-usage: "true"
    state.backend.rocksdb.metrics.block-cache-hit: "true"
    state.backend.rocksdb.metrics.block-cache-miss: "true"
    state.backend.rocksdb.metrics.bytes-read: "true"
    state.backend.rocksdb.metrics.bytes-written: "true"
    state.backend.rocksdb.metrics.column-family-as-variable: "true"
    state.backend.rocksdb.metrics.compaction-pending: "true"
    state.backend.rocksdb.metrics.compaction-read-bytes: "true"
    state.backend.rocksdb.metrics.compaction-write-bytes: "true"
    state.backend.rocksdb.metrics.cur-size-active-mem-table: "true"
    state.backend.rocksdb.metrics.cur-size-all-mem-tables: "true"
    state.backend.rocksdb.metrics.estimate-live-data-size: "true"
    state.backend.rocksdb.metrics.estimate-num-keys: "true"
    state.backend.rocksdb.metrics.estimate-pending-compaction-bytes: "true"
    state.backend.rocksdb.metrics.estimate-table-readers-mem: "true"
    #state.backend.rocksdb.metrics.is-write-stopped: "true"
    #state.backend.rocksdb.metrics.iter-bytes-read: "true"
    state.backend.rocksdb.metrics.live-sst-files-size: "true"
    #state.backend.rocksdb.metrics.mem-table-flush-pending: "true"
    #state.backend.rocksdb.metrics.num-deletes-active-mem-table: "true"
    #state.backend.rocksdb.metrics.num-deletes-imm-mem-tables: "true"
    #state.backend.rocksdb.metrics.num-entries-active-mem-table: "true"
    #state.backend.rocksdb.metrics.num-entries-imm-mem-tables: "true"
    #state.backend.rocksdb.metrics.num-immutable-mem-table: "true"
    #state.backend.rocksdb.metrics.num-live-versions: "true"
    #state.backend.rocksdb.metrics.num-running-compactions: "true"
    #state.backend.rocksdb.metrics.num-running-flushes: "true"
    #state.backend.rocksdb.metrics.num-snapshots: "true"
    state.backend.rocksdb.metrics.size-all-mem-tables: "true"
    #state.backend.rocksdb.metrics.stall-micros: "true"
    state.backend.rocksdb.metrics.total-sst-files-size: "true"

    state.backend.latency-track.keyed-state-enabled: "true"

  serviceAccount: flink
  podTemplate:
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-template
    spec:
      containers:
        # Do not change the main container name
        - name: flink-main-container
          volumeMounts:
            - mountPath: /opt/flink/downloads
              name: downloads
          ports:
            - containerPort: 9249
              name: prom
      volumes:
        - name: downloads
          emptyDir: { }
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1

    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        name: job-manager-pod-template
      spec:
        initContainers:
          # Sample init container for fetching remote artifacts
          - name: busybox
            image: busybox:1.35.0
            volumeMounts:
              - mountPath: /opt/flink/downloads
                name: downloads
            command:
              - /bin/sh
              - -c
              - "wget -O /opt/flink/downloads/job.jar \
                https://forge.uclouvain.be/DonatienSchmitz/justin/-/raw/main/WordCount.jar"
  taskManager:
    resource:
      memory: "4096m"
      cpu: 4
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        name: task-manager-pod-template
  job:
    jarURI: local:///opt/flink/downloads/job.jar
    parallelism: 1
    #upgradeMode: last-state

  logConfiguration:
    "log4j-console.properties": |
      rootLogger.level = DEBUG
      rootLogger.appenderRef.file.ref = LogFile
      rootLogger.appenderRef.console.ref = LogConsole
      appender.file.name = LogFile
      appender.file.type = File
      appender.file.append = false
      appender.file.fileName = ${sys:log.file}
      appender.file.layout.type = PatternLayout
      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
      appender.console.name = LogConsole
      appender.console.type = CONSOLE
      appender.console.layout.type = PatternLayout
      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
      logger.akka.name = akka
      logger.akka.level = INFO
      logger.kafka.name= org.apache.kafka
      logger.kafka.level = INFO
      logger.hadoop.name = org.apache.hadoop
      logger.hadoop.level = INFO
      logger.zookeeper.name = org.apache.zookeeper
      logger.zookeeper.level = INFO
      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
      logger.netty.level = OFF
